package org.apache.hadoop.hive.ql.exec.mr;
//Java code for Hive query for custom MapReduce Jobs
//Generated by HIVE compiler and packaged as part of job.jar that is passed to Hadoop to execute MapReduce job

/*
**	Packages imported:
**	Apache, java, slf4j, util
*/
import java.util.Map;
import org.apache.hadoop.hive.conf.HiveConf;
import org.slf4j.LoggerFactory;
import java.io.IOException;
import org.apache.hadoop.util.StringUtils;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.hive.ql.exec.OperatorUtils;
import java.util.Iterator;
import java.util.List;
import org.apache.hadoop.hive.ql.plan.MapWork;
import org.apache.hadoop.hive.ql.plan.OperatorDesc;
import org.apache.hadoop.hive.ql.exec.Operator;
import org.apache.hadoop.hive.ql.exec.MapredContext;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.ql.exec.MapOperator;
import org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator;
import org.apache.hadoop.hive.ql.CompilationOpContext;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hive.ql.exec.Utilities;
import java.util.Arrays;
import java.net.URLClassLoader;
import org.apache.hadoop.hive.ql.plan.MapredLocalWork;
import org.slf4j.Logger;
import org.apache.hadoop.mapred.Reporter;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.hive.ql.exec.AbstractMapOperator;
import org.apache.hadoop.mapred.Mapper;
import org.apache.hadoop.mapred.MapReduceBase;

public class ExecMapper extends MapReduceBase implements Mapper
{
    private AbstractMapOperator mo;
    private OutputCollector oc;
    private JobConf jc;
    private boolean abort;
    private Reporter rp;
    public static final Logger l4j;
    private static boolean done;
    private MapredLocalWork localWork;
    private ExecMapperContext execContext;
    
    public ExecMapper() {
        this.abort = false;
        this.localWork = null;
        this.execContext = null;
    }
    
	/*
	** Configure Map Job
	*/
    public void configure(final JobConf job) {
		//Job is an object for which its configuration
		//Remember, Hive determines the job plan before its executed in HDFS
        this.execContext = new ExecMapperContext(job);
        try {
            ExecMapper.l4j.info("conf classpath = " + Arrays.asList(((URLClassLoader)job.getClassLoader()).getURLs()));
            ExecMapper.l4j.info("thread classpath = " + Arrays.asList(((URLClassLoader)Thread.currentThread().getContextClassLoader()).getURLs()));
        }
        catch (Exception e) {
            ExecMapper.l4j.info("cannot get classpath: " + e.getMessage());
        }
		
		//Configure Job
        setDone(false);
        try {
            this.jc = job;
            this.execContext.setJc(this.jc);
            final MapWork mrwork = Utilities.getMapWork((Configuration)job);
            final CompilationOpContext runtimeCtx = new CompilationOpContext();
            if (mrwork.getVectorMode()) {
                this.mo = new VectorMapOperator(runtimeCtx);
            }
            else {
                this.mo = new MapOperator(runtimeCtx);
            }
            this.mo.setConf(mrwork);
            this.mo.initialize((Configuration)job, null);
            this.mo.setChildren((Configuration)job);
            ExecMapper.l4j.info(this.mo.dump(0));
            this.localWork = mrwork.getMapRedLocalWork();
            this.execContext.setLocalWork(this.localWork);
            MapredContext.init(true, new JobConf((Configuration)this.jc));
            this.mo.passExecContext(this.execContext);
            this.mo.initializeLocalWork((Configuration)this.jc);
            this.mo.initializeMapOperator((Configuration)this.jc);
            if (this.localWork == null) {
                return;
            }
            ExecMapper.l4j.info("Initializing dummy operator");
            final List<Operator<? extends OperatorDesc>> dummyOps = this.localWork.getDummyParentOp();
            for (final Operator<? extends OperatorDesc> dummyOp : dummyOps) {
                dummyOp.passExecContext(this.execContext);
                dummyOp.initialize((Configuration)this.jc, null);
            }
        }
        catch (Throwable e2) {
            this.abort = true;
            if (e2 instanceof OutOfMemoryError) {
                throw (OutOfMemoryError)e2;
            }
            throw new RuntimeException("Map operator initialization failed", e2);
        }
    }
    
	//Define Map class
	/*
	** map function that takes the key value pairs, initiallly empty but populates it with key value pairs.
	** provides updates via slf4j logger
	*/
    public void map(final Object key, final Object value, final OutputCollector output, final Reporter reporter) throws IOException {
        if (this.oc == null) {
            this.oc = output;
            this.rp = reporter;
            OperatorUtils.setChildrenCollector(this.mo.getChildOperators(), output);
            this.mo.setReporter(this.rp);
            MapredContext.get().setReporter(reporter);
        }
        this.execContext.resetRow();
        try {
            if (this.mo.getDone()) {
                ExecMapper.done = true;
            }
            else {
                this.mo.process((Writable)value);
            }
        }
        catch (Throwable e) {
            this.abort = true;
            if (e instanceof OutOfMemoryError) {
                throw (OutOfMemoryError)e;
            }
            ExecMapper.l4j.error(StringUtils.stringifyException(e));
            throw new RuntimeException(e);
        }
    }
    
    public void close() {
        if (this.oc == null) {
            ExecMapper.l4j.trace("Close called. no row processed by map.");
        }
        if (!this.abort) {
            this.abort = this.execContext.getIoCxt().getIOExceptions();
        }
        try {
            this.mo.close(this.abort);
            if (this.localWork != null) {
                final List<Operator<? extends OperatorDesc>> dummyOps = this.localWork.getDummyParentOp();
                for (final Operator<? extends OperatorDesc> dummyOp : dummyOps) {
                    dummyOp.close(this.abort);
                }
            }
            final ReportStats rps = new ReportStats(this.rp, (Configuration)this.jc);
            this.mo.preorderMap(rps);
        }
        catch (Exception e) {
            if (!this.abort) {
                ExecMapper.l4j.error("Hit error while closing operators - failing tree");
                throw new RuntimeException("Hive Runtime Error while closing operators", e);
            }
        }
        finally {
            MapredContext.close();
            Utilities.clearWorkMap((Configuration)this.jc);
        }
    }
    
    public static boolean getDone() {
        return ExecMapper.done;
    }
    
    public boolean isAbort() {
        return this.abort;
    }
    
    public void setAbort(final boolean abort) {
        this.abort = abort;
    }
    
    public static void setDone(final boolean done) {
        ExecMapper.done = done;
    }
    
    static {
        l4j = LoggerFactory.getLogger((Class)ExecMapper.class);
    }
    
    public static class ReportStats implements Operator.OperatorFunc
    {
        private final Reporter rp;
        private final String groupName;
        
        public ReportStats(final Reporter rp, final Configuration conf) {
            this.rp = rp;
            this.groupName = HiveConf.getVar(conf, HiveConf.ConfVars.HIVECOUNTERGROUP);
        }
        
        @Override
        public void func(final Operator op) {
            final Map<String, Long> opStats = (Map<String, Long>)op.getStats();
            for (final Map.Entry<String, Long> e : opStats.entrySet()) {
                if (this.rp != null) {
                    this.rp.incrCounter(this.groupName, (String)e.getKey(), (long)e.getValue());
                }
            }
        }
    }
}